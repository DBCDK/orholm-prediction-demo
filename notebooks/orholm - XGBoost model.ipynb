{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622856ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate absolute humidity\n",
    "def ah1(t, rh):\n",
    "    ah = (1322.9*(rh/100)*math.e**(t/(t+238.3)*17.2694) / (t+273.15))\n",
    "    return ah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf07cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read sheets from ScienceDirect into Pandas dataframe\n",
    "def rh_reader(fn, sheet):\n",
    "    df = pd.read_excel(fn, sheet_name=sheet,\n",
    "                          names=[\"date\", \"temp_in\", \"rh_in\"], header=None,\n",
    "                          parse_dates=True, index_col=0, skiprows=2, usecols=[0,1,2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e130c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from the three storage halls. Sheets come from ScienceDirect\n",
    "df_rh_1 = rh_reader('NM_S_OP_CI_CO_EN_2009-01_2020-12_002.xlsx', 'Ørholm-magasin-O-P-1-1-b')\n",
    "df_rh_2 = rh_reader('NM_S_OP2_CI_CO_EN_2009-01_2020-12_010.xlsx', 'Ørholm-magasin-O-P-1-2-b')\n",
    "df_rh_3 = rh_reader('NM_S_OP3_CI_2009-01_2021-05_016.xlsx', 'Ørholm-magasin-O-P-1-3-b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e62f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to avoid name clashes when we merge the dataframes\n",
    "df_rh_1.rename(columns={'temp_in': 'temp_1', 'rh_in': 'rh_1'}, inplace = True)\n",
    "df_rh_2.rename(columns={'temp_in': 'temp_2', 'rh_in': 'rh_2'}, inplace = True)\n",
    "df_rh_3.rename(columns={'temp_in': 'temp_3', 'rh_in': 'rh_3'}, inplace = True)\n",
    "\n",
    "# merge into one dataframe\n",
    "df = df_rh_1.join([df_rh_2, df_rh_3], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd98b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have three measurements that we assume should be somewhat similar.\n",
    "# To be roboust against \"drifting\" thermometers or hygrometers, we look at the median/middle one of the three.\n",
    "# we also calculate the \"middle absolute humidity\" for each.\n",
    "df['rh_median'] = df[['rh_1', 'rh_2', 'rh_3']].median(axis=1)\n",
    "df['temp_median'] = df[['temp_1', 'temp_2', 'temp_3']].median(axis=1)\n",
    "df['ah_median'] = df.apply(lambda x: ah1(x.temp_median, x.rh_median), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define auxillary values that are potentially interesting for time series analysis\n",
    "\n",
    "# define \"rolling mean\" of humidity over 24 hour periods\n",
    "df['rh_median_24_mean'] = df.rh_median.rolling(24, min_periods=1).mean()\n",
    "# define exponential weighted mean for humidity over 24 hr perionds\n",
    "df['rh_median_24_ewm_mean'] = df.rh_median.ewm(span=24).mean()\n",
    "# define medians of humidity over 24 hr periods\n",
    "df['rh_median_24_median'] = df.rh_median.rolling(24, min_periods=1).median()\n",
    "# Minimum and maximum values of humidity medians over 24 hour periods.\n",
    "df['rh_median_24_min'] = df.rh_median.rolling(24, min_periods=1).min()\n",
    "df['rh_median_24_max'] = df.rh_median.rolling(24, min_periods=1).max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aca4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction target: take mean of rolling 24-hr windows, and shift back a day.\n",
    "df['rh_median_mean_rolling'] = df.rh_median.rolling(window=24).mean().shift(-23)\n",
    "# if this gets too high or low, we will want to raise a flag, so y_low and y_high \n",
    "# will be the labels of our classifier.\n",
    "df['y_low'] = df['rh_median_mean_rolling'] < 40\n",
    "df['y_high'] = df['rh_median_mean_rolling'] > 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4021b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that labels are defined, we drop that column again\n",
    "df = df.drop(['rh_median_mean_rolling'], axis=1) \n",
    "# also drop original data; it is encoded in the aux columns\n",
    "df = df.drop(['temp_1', 'temp_2', 'temp_3', 'rh_1', 'rh_2', 'rh_3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost takes a numpy feature array as input. That will be the same for the 'high' and 'low' models\n",
    "features = df.copy()\n",
    "features = features.drop(['y_high', 'y_low'], axis=1)\n",
    "X = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568eaa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for both the 'high' and 'low' model, we do a grid search to find the best hyperparameters for \n",
    "# xgBoost classifier, and dump it to a file that we can subsequently use for predictions\n",
    "for s in ['y_high', 'y_low']:\n",
    "    # pick the right label\n",
    "    y = df[s].to_numpy()\n",
    "    # split data into train and test sets to build model.\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.25, random_state = 42, stratify=y)\n",
    "    param_fixed = {'objective':'binary:logistic', 'n_jobs':20}\n",
    "    clf = xgb.XGBClassifier(**param_fixed)\n",
    "    # these are the hyperparameters we do grid search on:\n",
    "    parameters = {\n",
    "        'max_depth' : range(2,10,1),\n",
    "        'n_estimators' : range(60,220,40),\n",
    "        'learning_rate': [0.1, 0.01, 0.05]\n",
    "    }\n",
    "    # this will take several hours, even on quite powerful machines. \n",
    "    # Consider running in a standalone program that you can leave running overnight.\n",
    "    grid = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=parameters,\n",
    "        scoring = 'roc_auc',\n",
    "        n_jobs = 10,\n",
    "        cv = 10,\n",
    "        verbose=True\n",
    "    ).fit(train_x, train_y)\n",
    "    # choose the best estimator from the grid search and predict on the test set:\n",
    "    best_pipe = grid.best_estimator_\n",
    "    predictions = best_pipe.predict(test_x)\n",
    "    # print the best hyperparams and evaluate model:\n",
    "    print(f\"best params: {grid.best_params_}\")\n",
    "    print(classification_report(test_y, predictions))\n",
    "    # dump the best model to model file that can subsequently be used to make predictions\n",
    "    joblib.dump(clf, f\"orholm-data-xgb-model-{s}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d0a0e",
   "metadata": {},
   "source": [
    "## Output (run from a standalone program)\n",
    "\n",
    "### y_high:\n",
    "\n",
    "```\n",
    "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
    "best params: {'learning_rate': 0.05, 'max_depth': 100, 'n_estimators': 500}\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99     22728\n",
    "           1       0.92      0.92      0.92      3297\n",
    "\n",
    "    accuracy                           0.98     26025\n",
    "   macro avg       0.95      0.95      0.95     26025\n",
    "weighted avg       0.98      0.98      0.98     26025\n",
    "```\n",
    "\n",
    "### y_low:\n",
    "\n",
    "```\n",
    "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
    "best params: {'learning_rate': 0.1, 'max_depth': 50, 'n_estimators': 500}\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00     24664\n",
    "           1       0.94      0.93      0.93      1361\n",
    "\n",
    "    accuracy                           0.99     26025\n",
    "   macro avg       0.97      0.96      0.96     26025\n",
    "weighted avg       0.99      0.99      0.99     26025\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069ab59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
